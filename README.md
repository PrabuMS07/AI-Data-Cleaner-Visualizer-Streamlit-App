

---

# AI-Powered Data Cleaner and Visualizer 📊🤖

This project provides a Streamlit web application that leverages a local Large Language Model (LLM) via Ollama to automate common data science tasks: dataset summarization, preprocessing, and visualization. Upload your CSV, and let the AI assist you in understanding and cleaning your data!

## ✨ Features

*   **⬆️ CSV Upload:** Easily upload your datasets via a simple drag-and-drop interface.
*   **📝 AI-Generated Summary:** Get an automatic description and key insights about your dataset's structure, missing values, and data types, generated by an LLM.
*   **🤖 Automated Preprocessing:**
    *   Generates Python code dynamically based on your dataset's characteristics.
    *   Handles missing values (numeric imputation with median, categorical with mode).
    *   Identifies and drops low-variance columns or columns with excessive missing data.
    *   Performs outlier detection and handling using the IQR method.
    *   Applies Label Encoding to categorical features.
    *   Scales numeric features using StandardScaler.
    *   Executes the generated code safely.
*   **📈 Automated Visualization:**
    *   Generates Python code to create relevant plots using Seaborn and Matplotlib.
    *   Creates histograms (with KDE) for numeric columns.
    *   Generates a correlation heatmap for numeric columns.
    *   Creates bar plots for categorical column value counts.
    *   Saves generated plots automatically.
*   **👀 Side-by-Side Comparison:** View your original dataset and the AI-cleaned dataset next to each other for easy comparison.
*   **💻 Code Display:** See the exact Python code generated by the LLM for preprocessing and visualization.
*   **🚀 Local LLM Powered:** Uses Ollama to run models like `qwen2.5:7b` locally, ensuring data privacy.

## 📸 Screenshots & Demo

**🎬 Demo Video:**

[Link to Demo Video](https://vimeo.com/1078426377?share=copy)


## ⚙️ How it Works

1.  **Upload:** The user uploads a CSV file via the Streamlit interface.
2.  **Analyze:** The application reads the CSV into a Pandas DataFrame.
3.  **Summarize:** A prompt containing dataset metadata (columns, dtypes, missing values, shape) is sent to the configured Ollama LLM endpoint. The LLM generates a textual summary and insights.
4.  **Preprocess:** A detailed prompt instructs the LLM to generate Python preprocessing code based on best practices (imputation, scaling, outlier handling, etc.), tailored dynamically to the dataset.
5.  **Execute (Preprocess):** The generated Python code is executed in a controlled environment, transforming the DataFrame. The cleaned data is saved.
6.  **Visualize:** Another prompt asks the LLM to generate Python code for creating standard visualizations (histograms, heatmaps, bar plots) using the *cleaned* data.
7.  **Execute (Visualize):** The visualization code is executed, saving plots as image files.
8.  **Display:** The Streamlit app displays the summary, generated code snippets, saved plot images, and a side-by-side view of the original and cleaned DataFrames.

## 🛠️ Technology Stack

*   **Frontend:** Streamlit
*   **Backend/Logic:** Python
*   **Data Handling:** Pandas, NumPy
*   **Machine Learning/Preprocessing:** Scikit-learn
*   **Visualization:** Matplotlib, Seaborn
*   **LLM Interaction:** Requests library, Ollama
*   **LLM:** Configured for `qwen2.5:7b` (or other Ollama-compatible models)

## 📋 Prerequisites

*   **Python:** Version 3.8 or higher.
*   **Ollama:** Must be installed and running locally. You can download it from [ollama.ai](https://ollama.ai/).
*   **LLM Model:** You need to have the specified model pulled in Ollama. Run:
    ```bash
    ollama pull qwen2.5:7b
    ```
    (Or replace `qwen2.5:7b` with your desired model if you change the script).
*   **pip:** Python package installer.

## 🚀 Setup and Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/your-username/your-repository-name.git
    cd your-repository-name
    ```
    *(Action: Replace `your-username/your-repository-name` with your actual GitHub details)*

2.  **Create `requirements.txt`:**
    Create a file named `requirements.txt` in the project's root directory with the following content:
    ```txt
    streamlit
    pandas
    scikit-learn
    seaborn
    matplotlib
    requests
    numpy
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Ensure Ollama is running:**
    Start the Ollama application or service if it's not already running. Verify it's accessible (usually at `http://localhost:11434`).

## ▶️ Running the Application

1.  Navigate to the project directory in your terminal.
2.  Run the Streamlit application:
    ```bash
    streamlit run AI_Data_Cleaner.py
    ```
3.  The application should automatically open in your web browser.

## ⚙️ Configuration

The script uses the following constants that you might want to adjust:

*   `OLLAMA_API_URL`: The URL for your running Ollama instance (default: `http://localhost:11434/api/generate`).
*   `MODEL_NAME`: The Ollama model tag to use (default: `qwen2.5:7b`). Make sure this model is pulled via `ollama pull <model_name>`.
*   `TEMP_DIR`: The directory where temporary files (uploaded dataset, cleaned dataset, plots, generated code) are stored (default: `temp`). This directory is created automatically.

## 🤝 Contributing

Contributions are welcome! If you have suggestions for improvements or find bugs, please feel free to open an issue or submit a pull request.

1.  Fork the Project
2.  Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3.  Commit your Changes (`git commit -m 'Add some AmazingFeature'`)
4.  Push to the Branch (`git push origin feature/AmazingFeature`)
5.  Open a Pull Request

